{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. get all fruits that the player can load either alone or with others\n",
    "2. for each of these fruits calculate summery statistics\n",
    "3. for each player make a prediciton for each of the fruits and save the result in a dictionary for each player. id fruits and prob\n",
    "4. given the first order believes about the othe players, choose the next target and give teh result as training back to teh prediction for the player\n",
    "5. When another player loads a fruit, train the neural network with the results by mapping it with the id. \n",
    "\n",
    "So the nn is trained to predict the reqard maximizing choice of a player given the choices of other players"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward = level_player * level_food creating a clear incentive to cooperate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lbforaging\n",
    "import gymnasium as gym\n",
    "from lbforaging.agents import H1\n",
    "import time\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from tr_lbf_addon.lbf_gym import Lbf_Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the enviromente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\envs\\registration.py:642: UserWarning: \u001b[33mWARN: Overriding environment Foraging-8x8-5p-8f-v3 already in registry.\u001b[0m\n",
      "  logger.warn(f\"Overriding environment {new_spec.id} already in registry.\")\n"
     ]
    }
   ],
   "source": [
    "# register the environment in gym\n",
    "\n",
    "field_size = 8 # size of the game board\n",
    "number_players = 5 # Number of players\n",
    "max_num_food = 8 # max amount of food on the board. TODO How is teh amount of food determined?\n",
    "coop_mode = False # If true, all foods will have teh max level so that all foods can only be loaded by working with other players\n",
    "max_episode_steps = 50 # Number of steps until one round (episode) is terminated\n",
    "sight = 0  #  How far can the agents see i presume TODO\n",
    "max_player_level = 1\n",
    "min_player_level = 1\n",
    "max_food_level = 1\n",
    "min_food_level = 1\n",
    "normalize_reward = True\n",
    "grid_observation = False\n",
    "observe_agent_levels = True # If true, the observation will include the level of the agents\n",
    "penalty = 0.0 # if the player was not the one to load the food, it gets a penalty to its reward\n",
    "render_mode = \"human\"\n",
    "full_info_mode = True\n",
    "\n",
    "id_string = \"Foraging-{0}x{0}-{1}p-{2}f{3}-v3\".format(field_size, number_players, max_num_food, \"-coop\" if coop_mode else \"\")\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=id_string,\n",
    "    entry_point=\"lbforaging.foraging:ForagingEnv\",\n",
    "    kwargs={\n",
    "        \"players\": number_players,\n",
    "        \"max_player_level\": max_player_level,\n",
    "        \"min_player_level\": min_player_level, \n",
    "        \"max_food_level\": max_food_level,\n",
    "        \"min_food_level\": min_food_level,\n",
    "        \"field_size\": (field_size, field_size),\n",
    "        \"max_num_food\": max_num_food,\n",
    "        \"sight\": sight,\n",
    "        \"max_episode_steps\": max_episode_steps,\n",
    "        \"force_coop\": coop_mode,\n",
    "        \"normalize_reward\" : normalize_reward,\n",
    "        \"grid_observation\" : grid_observation,\n",
    "        \"observe_agent_levels\" : observe_agent_levels,\n",
    "        \"penalty\" : penalty,\n",
    "        \"render_mode\" : render_mode,\n",
    "        \"full_info_mode\": full_info_mode\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:130: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\spaces\\box.py:418: UserWarning: \u001b[33mWARN: Casting input x to numpy array.\u001b[0m\n",
      "  gym.logger.warn(\"Casting input x to numpy array.\")\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:130: UserWarning: \u001b[33mWARN: The obs returned by the `reset()` method was expecting a numpy array, actual type: <class 'NoneType'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "# define the environment. A more detailed way is discribed on https://github.com/semitable/lb-foraging\n",
    "env = gym.make(id_string) # \"Foraging-{GRID_SIZE}x{GRID_SIZE}-{PLAYER COUNT}p-{FOOD LOCATIONS}f{-coop IF COOPERATIVE MODE}-v0\"\n",
    "# render_mode is \"human\" per default\n",
    "\n",
    "# reset the environment with a seed\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# initialize the class\n",
    "tr_marla_class = Lbf_Gym(observation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:130: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'dict'>\u001b[0m\n",
      "  logger.warn(\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:158: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method is not within the observation space.\u001b[0m\n",
      "  logger.warn(f\"{pre} is not within the observation space.\")\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:130: UserWarning: \u001b[33mWARN: The obs returned by the `step()` method was expecting a numpy array, actual type: <class 'NoneType'>\u001b[0m\n",
      "  logger.warn(\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:245: UserWarning: \u001b[33mWARN: The reward returned by `step()` must be a float, int, np.integer or np.floating, actual type: <class 'list'>\u001b[0m\n",
      "  logger.warn(\n",
      "s:\\coding_projects\\lbf_marl\\.venv\\Lib\\site-packages\\gymnasium\\utils\\passive_env_checker.py:265: UserWarning: \u001b[33mWARN: Human rendering should return `None`, got <class 'bool'>\u001b[0m\n",
      "  logger.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game lasted 10 steps\n"
     ]
    }
   ],
   "source": [
    "episode_over = False\n",
    "step_amount = 0\n",
    "while not episode_over:\n",
    "    tr_marla_class.update_observation(observation[0])\n",
    "    actions = tr_marla_class.agents_choose_actions()\n",
    "    observation, reward, terminated, truncated, info = env.step(tuple(actions))\n",
    "    # let 2 senconds pass\n",
    "    env.render()\n",
    "    time.sleep(0.2)\n",
    "    episode_over = terminated or truncated\n",
    "    step_amount += 1\n",
    "env.close()\n",
    "print(f\"Game lasted {step_amount} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_marla_class.update_observation(observation[0])\n",
    "actions = tr_marla_class.agents_choose_actions()\n",
    "observation, reward, terminated, truncated, info = env.step(tuple(actions))\n",
    "tr_marla_class.full_info_field"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# test neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from keras import models\n",
    "from keras import layers\n",
    "from keras import optimizers\n",
    "from keras import losses\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import tensorflow as tf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tets data\n",
    "\n",
    "number_of_players = 5\n",
    "fruit_level = 2\n",
    "min_fruit_level = 1\n",
    "max_fruit_level = 3\n",
    "id = 1\n",
    "ground_truth = np.array([1])\n",
    "\n",
    "known_agents = pd.DataFrame()\n",
    "known_agents[\"id\"] = [0,1,2,3,4]\n",
    "known_agents[\"level\"] = [1,1,2,2,3]\n",
    "known_agents[\"distance_to_fruit\"] = np.array([1, 2, 3, 4, 5])\n",
    "known_agents.set_index(\"id\", inplace=True)\n",
    "\n",
    "# normalize the levels with min max scaling\n",
    "max_level = known_agents[\"level\"].max()\n",
    "min_level = known_agents[\"level\"].min()\n",
    "known_agents[\"level\"] = (known_agents[\"level\"] - min_level) / (max_level - min_level)\n",
    "\n",
    "# normalize the distance to the fruit with min max scaling\n",
    "max_distance = known_agents[\"distance_to_fruit\"].max()\n",
    "min_distance = known_agents[\"distance_to_fruit\"].min()\n",
    "known_agents[\"distance_to_fruit\"] = (known_agents[\"distance_to_fruit\"] - min_distance) / (max_distance - min_distance)\n",
    "\n",
    "# normalize the fruit level with min max scaling\n",
    "fruit_level = (fruit_level - min_fruit_level) / (max_fruit_level - min_fruit_level)\n",
    "\n",
    "# prepare input\n",
    "known_agents.sort_index(inplace=True)\n",
    "agents_info = known_agents.loc[id]\n",
    "known_agents.drop(id, inplace=True)\n",
    "# first the level of the fruit than the agents info (level, distance to fruit) than the rest of the agents info first the level than the distance to the fruit each ordered by id\n",
    "input = np.array([fruit_level] + agents_info.tolist() + known_agents[\"level\"].tolist() + known_agents[\"distance_to_fruit\"].tolist())\n",
    "input = input.reshape(1,len(input))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Layer 0 weights have been updated.\n",
      "Layer 1 weights have been updated.\n",
      "Layer 2 weights have been updated.\n",
      "Layer 3 weights have been updated.\n"
     ]
    }
   ],
   "source": [
    "# Create a Sequential model\n",
    "model = models.Sequential()\n",
    "# Fit the normalizer on the training data\n",
    "model.add(layers.Input(shape=(11,)))\n",
    "model.add(layers.Dense(5, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "# model.compile(optimizer='adam', loss='mean_squared_error', metrics=['accuracy'])\n",
    "# Train the model\n",
    "\n",
    "loss_fn = losses.MeanSquaredError()\n",
    "optimizer = optimizers.Adam()\n",
    "\n",
    "initial_weights = model.get_weights() \n",
    "\n",
    "def learn(model, input, ground_truth):\n",
    "    with tf.GradientTape() as tape:\n",
    "        # Forward pass\n",
    "        prediction = model(input, training=True)\n",
    "        loss_value = loss_fn(ground_truth, prediction)\n",
    "        # caclulate the gradients\n",
    "        gradients = tape.gradient(loss_value, model.trainable_variables)\n",
    "    # Update the weights\n",
    "    optimizer.apply_gradients(zip(gradients, model.trainable_variables))\n",
    "    return model\n",
    "\n",
    "learn(model, input, ground_truth)\n",
    "updated_weights = model.get_weights()\n",
    "\n",
    "# chekc if the weights have been updated\n",
    "for i in range(len(initial_weights)):\n",
    "    if not tf.reduce_all(tf.equal(initial_weights[i], updated_weights[i])):\n",
    "        print(f\"Layer {i} weights have been updated.\")\n",
    "    else:\n",
    "        print(f\"Layer {i} weights have not been updated.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. reward ist immernoch nicht gelöst. Wenn der agent die frucht die gewinn maximierend ist wählt, berücksichtigt er nicht dass die Frucht auch von jemand anderen genommen werden kann bevor er sie erreicht. Daher muss er immer mit den nn predicten und erst evaluaten wenn er entweder die frucht hat oder die frucht weg ist. Daher muss das neuronale netz auch die belohnung vorhersagen. Der true value ist dann die belohnung von der frucht normalisiert oder 0 wenn die frucht schon weg ist. Aber wie unterscheidet man dann zwischen frucht wurde gewählt aber er ist nicht rechtzeitig hingekommen und er wollte die frucht garnicht? Muss man den nächssten reward von einem spieler vorhersagen und davon dann auf die frucht schließen? Q values sind dann wieder wichtig? Man könnte einfgach öffentlich machen welche frucht wer wählt. Das wäre das einfache. Oder man evaluiert nur die vorhersagen die vorhersage richtig warund da wo man es nicht weiß gibt man 0.5 an oder so. Zuerst das einfach edann das schwierige machen. \n",
    "\n",
    "2. es muss noch eine logik eingeführt werden um zu wissen welche frucht wer gewählt hat wenn jemand einen reward bekommt. Die reards sind einfach eine liste und müssen erst noch zu den agent_infos in den observations in jeder runde hinzugefügt werden. Um zu wissen welche fruit von wem gelootet wurde kan bei denen die einen reward haben geschaut werden wo sie stehen und welche fruit zuvor da stand. Dafür muss die field creation überarbeitet werden sodass das \"alte\" field gespeichert wird um immer vergleichen zu können. also erst rewards in player info und altes field immer speichern und dann wenn ein agent ein reward bekommt schauen wo er steht und welche fruit er genommen hat. Vielleicht muss auch nicht das alte feld gespeichert werden sondern das ganze bevor das field überschrieben wird gemacht werden. \n",
    "\n",
    "3. implement teh learning function into the project\n",
    "\n",
    "4. maybe but cognition in seperate class that agents inherits. Makes things tidier\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
