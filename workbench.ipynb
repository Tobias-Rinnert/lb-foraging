{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Description "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- reward = level_player * level_food creating a clear incentive to cooperate\n",
    "- learning to navigate the world is not needed. One could simplify use the action space: Of all available foods choose one where to go given its location, level and the position of the other players. Would make interpretation better as one could see which food was targeted. First progeam a basic rl algorithm (look into marllib). "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lbforaging\n",
    "import gymnasium as gym\n",
    "from lbforaging.agents import H1\n",
    "import time\n",
    "import numpy as np\n",
    "\n",
    "from tr_lbf_addon.lbf_gym import Lbf_Gym"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create the enviromente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# register the environment in gym\n",
    "\n",
    "field_size = 8 # size of the game board\n",
    "number_players = 2 # Number of players\n",
    "max_num_food = 8 # max amount of food on the board. TODO How is teh amount of food determined?\n",
    "coop_mode = False # If true, all foods will have teh max level so that all foods can only be loaded by working with other players\n",
    "max_episode_steps = 50 # Number of steps until one round (episode) is terminated\n",
    "sight = 0  #  How far can the agents see i presume TODO\n",
    "max_player_level = 1\n",
    "min_player_level = 1\n",
    "max_food_level = 1\n",
    "min_food_level = 1\n",
    "normalize_reward = True\n",
    "grid_observation = False\n",
    "observe_agent_levels = True # If true, the observation will include the level of the agents\n",
    "penalty = 0.0 # if the player was not the one to load the food, it gets a penalty to its reward\n",
    "render_mode = \"human\"\n",
    "full_info_mode = True\n",
    "\n",
    "id_string = \"Foraging-{0}x{0}-{1}p-{2}f{3}-v3\".format(field_size, number_players, max_num_food, \"-coop\" if coop_mode else \"\")\n",
    "\n",
    "gym.envs.registration.register(\n",
    "    id=id_string,\n",
    "    entry_point=\"lbforaging.foraging:ForagingEnv\",\n",
    "    kwargs={\n",
    "        \"players\": number_players,\n",
    "        \"max_player_level\": max_player_level,\n",
    "        \"min_player_level\": min_player_level, \n",
    "        \"max_food_level\": max_food_level,\n",
    "        \"min_food_level\": min_food_level,\n",
    "        \"field_size\": (field_size, field_size),\n",
    "        \"max_num_food\": max_num_food,\n",
    "        \"sight\": sight,\n",
    "        \"max_episode_steps\": max_episode_steps,\n",
    "        \"force_coop\": coop_mode,\n",
    "        \"normalize_reward\" : normalize_reward,\n",
    "        \"grid_observation\" : grid_observation,\n",
    "        \"observe_agent_levels\" : observe_agent_levels,\n",
    "        \"penalty\" : penalty,\n",
    "        \"render_mode\" : render_mode,\n",
    "        \"full_info_mode\": full_info_mode\n",
    "        },\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the environment. A more detailed way is discribed on https://github.com/semitable/lb-foraging\n",
    "env = gym.make(id_string) # \"Foraging-{GRID_SIZE}x{GRID_SIZE}-{PLAYER COUNT}p-{FOOD LOCATIONS}f{-coop IF COOPERATIVE MODE}-v0\"\n",
    "# render_mode is \"human\" per default\n",
    "\n",
    "# reset the environment with a seed\n",
    "observation, info = env.reset(seed=42)\n",
    "\n",
    "# initialize the class\n",
    "tr_marla_class = Lbf_Gym(observation[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tests"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "the goal now is to first replicate the tsimple heuristic, moving teh agent to to a given target. \n",
    "Then deploy a neural network that takes as its input the state of teh game and the position of the othe players and returns the fruit to go to. Output of the neural network is the fruit the agent should go to. Problem: How to label the fruit so that the neural network can learn over the course of several geames. Possibility. Output layer size = number of max fruit. Input is also the mapping giving each fruit in the current state a number. Then iterative: Go through each fruit and rate it with a nukber between 0 and 1. Later take the fruit with teh highest score. If two fruits have the same score choose randomely between them. Has the benefit that you can give summary statistics as input as well, which the neural network does not have to learn. So input is the position of the player, the game sioze, the position of the fruits, the distance between each fruit to another fruit and so on. This way teh neural network learns the correlation between these values and rates each fruit. Question: Does this acoount for cooperation well enough? Should. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The astar pathfinding sometimes does not find a path for no reason todo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Game lasted 13 steps\n"
     ]
    }
   ],
   "source": [
    "episode_over = False\n",
    "step_amount = 0\n",
    "while not episode_over:\n",
    "    tr_marla_class.update_observation(observation[0])\n",
    "    actions = tr_marla_class.agents_choose_actions()\n",
    "    observation, reward, terminated, truncated, info = env.step(tuple(actions))\n",
    "    # let 2 senconds pass\n",
    "    env.render()\n",
    "    time.sleep(0.2)\n",
    "    episode_over = terminated or truncated\n",
    "    step_amount += 1\n",
    "env.close()\n",
    "print(f\"Game lasted {step_amount} steps\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tr_marla_class.update_observation(observation[0])\n",
    "actions = tr_marla_class.agents_choose_actions()\n",
    "observation, reward, terminated, truncated, info = env.step(tuple(actions))\n",
    "tr_marla_class.full_info_field"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv (3.12.8)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
